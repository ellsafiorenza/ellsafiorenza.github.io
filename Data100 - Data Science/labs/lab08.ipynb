{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "Assignment: lab08\n",
      "OK, version v1.13.11\n",
      "=====================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize OK\n",
    "from client.api.notebook import Notebook\n",
    "ok = Notebook('lab08.ok')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you will go through the practice of using regularization to reduce overfitting. \n",
    "\n",
    "### Due Date ###\n",
    "The assignment is due on **Monday, April 13th at 11:59pm PST**.\n",
    "\n",
    "### Collaboration Policy\n",
    "Data science is a collaborative activity. While you may talk with others about this assignment, we ask that you **write your solutions individually**. If you discuss the assignment with others, please **include their names** in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Collaborators:** *I work alone.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to set up your notebook\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.linear_model as lm\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from IPython.display import display, Latex, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use the same dataset as in Lab 7. Run the following cell to load the data into a dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston_data = load_boston()\n",
    "#print(boston_data['DESCR']) # Uncomment this line if you are interested in what the features are\n",
    "\n",
    "boston = pd.DataFrame(boston_data['data'], columns=boston_data['feature_names'])\n",
    "boston.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that in Lab 7, we started with a linear model using the full set of features from the `boston` dataframe, expanded the feature set by adding the square, square root, and hyperbolic tangent of every feature in the original dataframe, and used cross-validation to select the best first $N$ features from the expanded feature set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please run the following cell to expand the feature set by adding the square, square root, and hyperbolic tangent of every feature in the original dataframe like we did in Lab 7. Now the feature matrix has 52 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>...</th>\n",
       "      <th>tanhTAX</th>\n",
       "      <th>PTRATIO^2</th>\n",
       "      <th>sqrtPTRATIO</th>\n",
       "      <th>tanhPTRATIO</th>\n",
       "      <th>B^2</th>\n",
       "      <th>sqrtB</th>\n",
       "      <th>tanhB</th>\n",
       "      <th>LSTAT^2</th>\n",
       "      <th>sqrtLSTAT</th>\n",
       "      <th>tanhLSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>234.09</td>\n",
       "      <td>3.911521</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157529.6100</td>\n",
       "      <td>19.922349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.8004</td>\n",
       "      <td>2.231591</td>\n",
       "      <td>0.999905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>316.84</td>\n",
       "      <td>4.219005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157529.6100</td>\n",
       "      <td>19.922349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.5396</td>\n",
       "      <td>3.023243</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>316.84</td>\n",
       "      <td>4.219005</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154315.4089</td>\n",
       "      <td>19.819939</td>\n",
       "      <td>1.0</td>\n",
       "      <td>16.2409</td>\n",
       "      <td>2.007486</td>\n",
       "      <td>0.999368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>349.69</td>\n",
       "      <td>4.324350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>155732.8369</td>\n",
       "      <td>19.865296</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.6436</td>\n",
       "      <td>1.714643</td>\n",
       "      <td>0.994426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>349.69</td>\n",
       "      <td>4.324350</td>\n",
       "      <td>1.0</td>\n",
       "      <td>157529.6100</td>\n",
       "      <td>19.922349</td>\n",
       "      <td>1.0</td>\n",
       "      <td>28.4089</td>\n",
       "      <td>2.308679</td>\n",
       "      <td>0.999953</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  ...  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0  ...   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0  ...   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0  ...   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0  ...   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0  ...   \n",
       "\n",
       "   tanhTAX  PTRATIO^2  sqrtPTRATIO  tanhPTRATIO          B^2      sqrtB  \\\n",
       "0      1.0     234.09     3.911521          1.0  157529.6100  19.922349   \n",
       "1      1.0     316.84     4.219005          1.0  157529.6100  19.922349   \n",
       "2      1.0     316.84     4.219005          1.0  154315.4089  19.819939   \n",
       "3      1.0     349.69     4.324350          1.0  155732.8369  19.865296   \n",
       "4      1.0     349.69     4.324350          1.0  157529.6100  19.922349   \n",
       "\n",
       "   tanhB  LSTAT^2  sqrtLSTAT  tanhLSTAT  \n",
       "0    1.0  24.8004   2.231591   0.999905  \n",
       "1    1.0  83.5396   3.023243   1.000000  \n",
       "2    1.0  16.2409   2.007486   0.999368  \n",
       "3    1.0   8.6436   1.714643   0.994426  \n",
       "4    1.0  28.4089   2.308679   0.999953  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston_with_extra_features = boston.copy()\n",
    "for feature_name in boston.columns:\n",
    "    boston_with_extra_features[feature_name + \"^2\"] = boston_with_extra_features[feature_name] ** 2\n",
    "    boston_with_extra_features[\"sqrt\" + feature_name] = np.sqrt(boston_with_extra_features[feature_name])\n",
    "    boston_with_extra_features[\"tanh\" + feature_name] = np.tanh(boston_with_extra_features[feature_name])\n",
    "    \n",
    "boston_with_extra_features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = boston_with_extra_features\n",
    "Y = boston_data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, recall from Lab 7 that when we plot the training and test error as we add each additional feature, our model begins to overfit. That is, even though the training error continues to decrease, the test error starts to increase. This is undesirable as the goal of machine learning is that our model should generalize well to unseen data.\n",
    "\n",
    "In Lab 7, we used cross validation to find the best \"first $N$\" features. One drawback of this method is that we are not considering every subset of the features. In fact, we only compared the perfomance of models trained from 52 different feature sets. We can in theory use cross-validation on all possible subsets of the $52$ features, which will involving doing cross-validation on\n",
    "$$2^{52} - 1 \\approx 4.5 \\times 10^{15}$$  \n",
    "different feature sets, which can be computationally challenging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alteratively, we can use lasso regularization to perform automatic feature selection. And in general, we can use regularization to avoid overfitting. The idea of regularization is to penalize the weights $\\theta$ corresponding to the features through some function $Reg(\\theta)$ in the loss. Hence our objective is to compute the following:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg \\min\\limits_{\\theta} \\frac{1}{n}\\sum\\limits_{i=1}^{n} Loss(Y_i, f_{\\theta}(X_i)) + \\alpha Reg(\\theta)\n",
    "$$\n",
    "where $\\alpha$ is a hyperparameter. \n",
    "\n",
    "In the following questions we will examine two commonly used regularization functions ($d$ is the number of features used):\n",
    "\n",
    "(1) Lasso: $\\text{Reg}^{\\text{lasso}}(\\theta) = \\sum_{i=1}^{d} |\\theta_i|$\n",
    "\n",
    "(2) Ridge: $\\text{Reg}^{\\text{ridge}}(\\theta) = \\sum_{i=1}^{d} \\theta_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 1\n",
    "\n",
    "Before we fit a linear model with regularization, we need to first normalize our features. This is because if different features are on different magnitude scales, the magnitude of the corresponding weights for a good model may be very different. However, we penalize the weight for each feature equally in regularization. \n",
    "\n",
    "You can standardize a feature by computing: \n",
    "$$\n",
    "z = \\frac{x - \\text{Mean}(x)}{\\text{StdDev}(x)}\n",
    "$$\n",
    "\n",
    "In the cell below, complete the `normalize` function which returns the standardized version of input `data`. If $\\text{StdDev}(x)$ is $0$ for a feature, fill in $0$'s for the standardized values for this feature.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q1\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>...</th>\n",
       "      <th>tanhTAX</th>\n",
       "      <th>PTRATIO^2</th>\n",
       "      <th>sqrtPTRATIO</th>\n",
       "      <th>tanhPTRATIO</th>\n",
       "      <th>B^2</th>\n",
       "      <th>sqrtB</th>\n",
       "      <th>tanhB</th>\n",
       "      <th>LSTAT^2</th>\n",
       "      <th>sqrtLSTAT</th>\n",
       "      <th>tanhLSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419367</td>\n",
       "      <td>0.284548</td>\n",
       "      <td>-1.286636</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.144075</td>\n",
       "      <td>0.413263</td>\n",
       "      <td>-0.119895</td>\n",
       "      <td>0.140075</td>\n",
       "      <td>-0.981871</td>\n",
       "      <td>-0.665949</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.456987</td>\n",
       "      <td>-1.452136</td>\n",
       "      <td>0.134676</td>\n",
       "      <td>0.525807</td>\n",
       "      <td>0.384010</td>\n",
       "      <td>0.046184</td>\n",
       "      <td>-0.788749</td>\n",
       "      <td>-1.201500</td>\n",
       "      <td>0.103427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.416927</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>0.194082</td>\n",
       "      <td>0.366803</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.372709</td>\n",
       "      <td>-0.266657</td>\n",
       "      <td>0.178549</td>\n",
       "      <td>0.525807</td>\n",
       "      <td>0.384010</td>\n",
       "      <td>0.046184</td>\n",
       "      <td>-0.539919</td>\n",
       "      <td>-0.399557</td>\n",
       "      <td>0.128269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.416929</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-0.592794</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.739530</td>\n",
       "      <td>1.281446</td>\n",
       "      <td>-0.265549</td>\n",
       "      <td>0.556609</td>\n",
       "      <td>-0.867024</td>\n",
       "      <td>-0.986353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.372709</td>\n",
       "      <td>-0.266657</td>\n",
       "      <td>0.178549</td>\n",
       "      <td>0.448969</td>\n",
       "      <td>0.355550</td>\n",
       "      <td>0.046184</td>\n",
       "      <td>-0.825008</td>\n",
       "      <td>-1.428519</td>\n",
       "      <td>-0.037809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416338</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.015298</td>\n",
       "      <td>-0.809088</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057726</td>\n",
       "      <td>0.139493</td>\n",
       "      <td>0.178788</td>\n",
       "      <td>0.482854</td>\n",
       "      <td>0.368155</td>\n",
       "      <td>0.046184</td>\n",
       "      <td>-0.857192</td>\n",
       "      <td>-1.725169</td>\n",
       "      <td>-1.337326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412074</td>\n",
       "      <td>-0.487240</td>\n",
       "      <td>-1.305586</td>\n",
       "      <td>-0.272329</td>\n",
       "      <td>-0.834458</td>\n",
       "      <td>1.227362</td>\n",
       "      <td>-0.510674</td>\n",
       "      <td>1.076671</td>\n",
       "      <td>-0.752178</td>\n",
       "      <td>-1.105022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.057726</td>\n",
       "      <td>0.139493</td>\n",
       "      <td>0.178788</td>\n",
       "      <td>0.525807</td>\n",
       "      <td>0.384010</td>\n",
       "      <td>0.046184</td>\n",
       "      <td>-0.773463</td>\n",
       "      <td>-1.123410</td>\n",
       "      <td>0.115936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 52 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       CRIM        ZN     INDUS      CHAS       NOX        RM       AGE  \\\n",
       "0 -0.419367  0.284548 -1.286636 -0.272329 -0.144075  0.413263 -0.119895   \n",
       "1 -0.416927 -0.487240 -0.592794 -0.272329 -0.739530  0.194082  0.366803   \n",
       "2 -0.416929 -0.487240 -0.592794 -0.272329 -0.739530  1.281446 -0.265549   \n",
       "3 -0.416338 -0.487240 -1.305586 -0.272329 -0.834458  1.015298 -0.809088   \n",
       "4 -0.412074 -0.487240 -1.305586 -0.272329 -0.834458  1.227362 -0.510674   \n",
       "\n",
       "        DIS       RAD       TAX  ...  tanhTAX  PTRATIO^2  sqrtPTRATIO  \\\n",
       "0  0.140075 -0.981871 -0.665949  ...      0.0  -1.456987    -1.452136   \n",
       "1  0.556609 -0.867024 -0.986353  ...      0.0  -0.372709    -0.266657   \n",
       "2  0.556609 -0.867024 -0.986353  ...      0.0  -0.372709    -0.266657   \n",
       "3  1.076671 -0.752178 -1.105022  ...      0.0   0.057726     0.139493   \n",
       "4  1.076671 -0.752178 -1.105022  ...      0.0   0.057726     0.139493   \n",
       "\n",
       "   tanhPTRATIO       B^2     sqrtB     tanhB   LSTAT^2  sqrtLSTAT  tanhLSTAT  \n",
       "0     0.134676  0.525807  0.384010  0.046184 -0.788749  -1.201500   0.103427  \n",
       "1     0.178549  0.525807  0.384010  0.046184 -0.539919  -0.399557   0.128269  \n",
       "2     0.178549  0.448969  0.355550  0.046184 -0.825008  -1.428519  -0.037809  \n",
       "3     0.178788  0.482854  0.368155  0.046184 -0.857192  -1.725169  -1.337326  \n",
       "4     0.178788  0.525807  0.384010  0.046184 -0.773463  -1.123410   0.115936  \n",
       "\n",
       "[5 rows x 52 columns]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(data):\n",
    "    '''\n",
    "    Args:\n",
    "        data : a dataframe\n",
    "    Returns:\n",
    "        the normalized version of input data with NAN values filled with 0's\n",
    "    '''\n",
    "    return ((data - data.mean()) / data.std()).fillna(0)\n",
    "\n",
    "X_normalized = normalize(X)\n",
    "X_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 3\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q1\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we asked you to implement the normalize function yourself above to help you gain familiarity with what is means by normalizing the data. However, for the rest of the lab, we will use the [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) function from sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will split the dataset into training set and test set. Then we compute the normalized version of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train & Test split\n",
    "np.random.seed(41) # Do not change this line\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.10)\n",
    "X_train_normalized = StandardScaler().fit_transform(X_train)\n",
    "X_test_normalized = StandardScaler().fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use ridge regression and assume our loss is $L_2$ loss, we solve the following optimization problem: \n",
    "$$\n",
    "\\hat{\\theta} = \\arg \\min\\limits_{\\theta} \\frac{1}{n}\\sum\\limits_{i=1}^{n} L_2(Y_i, f_{\\theta}(X_i)) + \\alpha \\sum\\limits_{k=1}^{d}\\theta_k^2.\n",
    "$$\n",
    "where $\\alpha$ is a nonnegative hyper-parameter. \n",
    "\n",
    "One nice thing about ridge linear regression with L-2 loss is that it admits a closed form solution even if $X^{\\top}X$ is not full rank. The closed form solution is given by\n",
    "$$\n",
    "\\hat{\\theta} = (X^{\\top}X + n\\alpha I)^{-1}X^{\\top}Y\n",
    "$$\n",
    "for the above optimization problem.\n",
    "\n",
    "To evaluate model performance, we define RMSE in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse_score(model, X, y):\n",
    "    return np.sqrt(np.mean((y - model.predict(X)) ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2a\n",
    "\n",
    "Use `sklearn.linear_model.ridge` to fit ridge linear regression models and compute the RMSE using \n",
    "**5-fold** cross-validation. \n",
    "\n",
    "**Note: the data should be normalized after splitting the data in general.** For this question, to normalize the data, please use the function [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html). Please use this function in combination with [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html), and function `rmse_score` defined previously to complete the code in the cell below. \n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2a\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation RMSE: 14.615491801879747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "alpha = 0.1 # The regularization hyperparameter\n",
    "\n",
    "# Initiate a model with the hyperparameter `alpha`\n",
    "# Hint: the syntax should look like: Pipeline([(\"transformer\", ..., (\"LinearModel\", ...)])\n",
    "model = Pipeline([(\"transformer\", StandardScaler()), (\"LinearModel\", Ridge(alpha))])\n",
    "\n",
    "# Evaluate the RMSE of the validation error\n",
    "# Hint: Don't forget to take the mean!\n",
    "validation_error = cross_val_score(model, X_train, Y_train, scoring = rmse_score, cv = 5).mean()\n",
    "\n",
    "\n",
    "print(\"Validation RMSE:\", validation_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2a\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter in Ridge Linear Regression\n",
    "\n",
    "We just fitted models using ridge regression! However, our choice of hyperparameter seems rather arbitrary. \n",
    "\n",
    "In the following questions, we will experiment with different choices of $\\alpha$ and look at the effect of different values of $\\alpha$ on the weights. \n",
    "\n",
    "For your convenience, we define a function `ridge_alpha` which returns the weights $\\theta$ of the fitted linear model given standardized data `X` and `y` with ridge regression and the RMSE on the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_alpha(alpha, X, y):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        X - standardized feature matrix\n",
    "        y - target vector\n",
    "        alpha - regularization hyper-parameter\n",
    "    Output:\n",
    "        The tuple: (model_weights, rmse) where model_weights is d-dimensional array, each element \n",
    "        is rounded to the 3rd decimal. d is the dimension of each datapoint, or equivalently, the number \n",
    "        of features.\n",
    "    \"\"\"\n",
    "    model = Ridge(alpha = alpha)\n",
    "    model.fit(X, y)\n",
    "    error = rmse_score(model, X, y)\n",
    "\n",
    "    return (np.round(model.coef_, decimals = 3), error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first examine the output of ridge regression for extreme values of $\\alpha$. \n",
    "\n",
    "To start, let's consider the case where $\\alpha = 0$. In this case, the ridge linear regression objective function:\n",
    "$$\n",
    "\\frac{1}{n}\\sum\\limits_{i=1}^{n} L_2(Y_i, f_{\\theta}(X_i)) + \\alpha \\sum\\limits_{i=1}^{d}\\theta_k^2\n",
    "$$\n",
    "becomes\n",
    "$$\n",
    "\\frac{1}{n}\\sum\\limits_{i=1}^{n} L_2(Y_i, f_{\\theta}(X_i)),\n",
    "$$\n",
    "which is identical to the loss function for ordinary least squares. Hence, in this case, the linear model we fit will be identical to the one without regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2b\n",
    "\n",
    "### Question 2b(i)\n",
    "Now let's consider the case where $\\alpha$ is very large. In the cell below, compute the weights and RMSE of the ridge regression model when $\\alpha = 10^{10}$.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2bi\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights: [-0.  0. -0.  0. -0.  0. -0. -0.  0. -0. -0.  0. -0. -0. -0.  0.  0.  0.\n",
      "  0. -0. -0. -0.  0.  0.  0. -0. -0. -0.  0.  0.  0. -0. -0. -0. -0. -0.\n",
      " -0.  0.  0.  0. -0. -0.  0. -0. -0. -0.  0.  0. -0. -0. -0. -0.]\n",
      "RMSE: 7.945576175109449\n"
     ]
    }
   ],
   "source": [
    "ridge_alpha_10e10_weights, ridge_alpha_10e10_rmse = ridge_alpha(10**10, X, Y)\n",
    "print(\"weights:\", ridge_alpha_10e10_weights)\n",
    "print(\"RMSE:\", ridge_alpha_10e10_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2bi\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Question 2b(ii)\n",
    "\n",
    "What do you observe from the result above? Justify your observation. Write your answers in the cell below:\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2bii\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A large alpha makes the model does not fit very well. Increasing the alpha value will make the error to go up and validation error to go up slightly which suggest the model starts to underfit as we increase the alpha.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2c "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the cell below, compute the **5-fold** cross validation error using the sklearn functionalities introduced in lecture for all the values in `alpha_arr`. Then find the $\\alpha$ with the smallest cross-validation error.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2c\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha value is 0.15830508474576271\n",
      "Cross validation error for the best alpha value is 14.481714562577206\n"
     ]
    }
   ],
   "source": [
    "alpha_arr = np.linspace(0.02, 0.5, 60)\n",
    "cv_errors = []\n",
    "\n",
    "# Hint: This should be very similar to what you did in 2a.\n",
    "\n",
    "model = Pipeline([(\"transformer\", StandardScaler()), (\"LinearModel\", Ridge(alpha))])\n",
    "\n",
    "for alpha in alpha_arr:\n",
    "    model.set_params(LinearModel__alpha=alpha)\n",
    "        \n",
    "    # compute the cross validation error\n",
    "    cv_error = cross_val_score(model, X_train, Y_train, scoring = rmse_score, cv = 5).mean()\n",
    "    \n",
    "    cv_errors.append(cv_error)\n",
    "    \n",
    "best_alpha_ridge = alpha_arr[np.argmin(cv_errors)]\n",
    "\n",
    "print(f\"The best alpha value is {best_alpha_ridge}\")\n",
    "print(f\"Cross validation error for the best alpha value is {cv_errors[np.argmin(cv_errors)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q2c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 2d\n",
    "In the cell below, plot the cross validation errors for different values of $\\alpha$ for ridge regression. The x-axis should be $\\alpha$ and y-axis should be the cross validation error.\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q2d\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x = alpha_arr, y = cv_errors, marker = '.')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('CV Errors')\n",
    "plt.title('CV Errors vs. alpha')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe from the plot above? Write your answer in the cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The CV Errors vs. alpha graph looks like a skewed parabola. If we use a very small value of alpha or a large value of alpha, then the errors will increase. Thus, there is a best alpha value with the smallest cv error.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression\n",
    "\n",
    "If we use lasso regression and assume our loss is $L_2$ loss, we solve the following optimization problem: \n",
    "\n",
    "$$\n",
    "\\hat{\\theta} = \\arg \\min\\limits_{\\theta} \\frac{1}{n}\\sum\\limits_{i=1}^{n} L_2(Y_i, f_{\\theta}(X_i)) + \\alpha \\sum\\limits_{k=1}^{d}|\\theta_k|.\n",
    "$$\n",
    "\n",
    "One nice thing about lasso regression is that we can use it to select features because optimized weights for lasso regression are sparse, i.e., many of the weights are $0$'s. For more details on why this is true, please refer to Lecture 19. \n",
    "\n",
    "<img src=\"lasso.png\" width=500px>\n",
    "\n",
    "You can inspect the weights of a fitted lasso linear regression model in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00000000e+00,  0.00000000e+00, -0.00000000e+00,  1.89814435e-04,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  9.58388276e-02,  2.49861182e-16,\n",
       "        1.24930591e-16, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "        2.81742915e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00,  0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00, -0.00000000e+00,\n",
       "       -1.00827425e+00, -0.00000000e+00,  0.00000000e+00,  2.33415288e-01,\n",
       "        0.00000000e+00, -0.00000000e+00, -4.09722458e+00, -2.25533411e-02])"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = Lasso(alpha = 1)\n",
    "model.fit(X_train_normalized, Y_train)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, the weights of a fitted ridge linear regression model looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.26302901e+00, -2.06272043e+00,  1.11764381e+00,  1.38970319e-01,\n",
       "       -7.95402029e-01, -5.74045140e-01, -2.70520013e-01, -1.91484640e+00,\n",
       "        7.44555761e-01, -3.65231594e-01, -9.75774607e-01,  2.28463264e+00,\n",
       "        2.26263963e-01,  1.68717462e+00, -2.30814156e+00,  1.46007305e+00,\n",
       "        2.48199791e+00, -2.14749594e+00,  2.01041940e+00, -4.66948027e-02,\n",
       "       -1.50857443e+00, -3.45536581e-01,  1.38970319e-01,  1.38970319e-01,\n",
       "        1.38970319e-01, -2.52626196e+00,  4.85999267e-01,  2.72109452e-01,\n",
       "        8.74126822e+00, -5.37435186e+00,  3.72344988e-03, -2.13791478e-01,\n",
       "        7.16080895e-02,  1.80952178e-01,  3.08785667e-01, -2.00700353e-01,\n",
       "       -2.42642002e+00,  1.13107008e+00,  9.20301467e-01,  3.93556792e-01,\n",
       "        1.72867532e+00, -3.04877972e+00,  0.00000000e+00,  1.80178413e+00,\n",
       "       -2.42001210e+00,  5.92677955e-02, -1.91213877e+00, -9.27715887e-04,\n",
       "        3.07779127e-02,  6.54127461e-01, -5.05208413e+00, -4.66420136e-01])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Ridge(alpha = 1)\n",
    "model.fit(X_train_normalized, Y_train)\n",
    "model.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter in Lasso Linear Regression\n",
    "\n",
    "Similar to ridge regression, as $\\alpha$ goes to zero, lasso regression becomes identical to ordinary linear regression without regularization; as $\\alpha$ goes to infinity, all the fitted weights become zero.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 3\n",
    "Just like what we did for ridge regression, we need to tune the hyperparameter $\\alpha$ to find the best $\\alpha$ for lasso regression. In the cell below, use **5-fold** cross validation to find the optimal parameter $\\alpha$.\n",
    "\n",
    "**Note: When initiating your Lasso Linear Regression model, put `max_iter = 1000000` as an argument to `Lasso()`. The reason behind this is that for small regularization parameters, the Lasso Linear Regularization solver takes more iterations to converge.**\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q3\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best alpha value is 0.12372881355932204\n",
      "Cross validation error for the best alpha value is 4.350384165146851\n"
     ]
    }
   ],
   "source": [
    "alpha_arr = np.linspace(0.1, 1.5, 60)\n",
    "cv_errors = []\n",
    "model = Pipeline([(\"transformer\", StandardScaler()), \n",
    "                  (\"LinearModel\", Lasso(alpha, max_iter = 1000000))\n",
    "                 ])\n",
    "\n",
    "for alpha in alpha_arr:\n",
    "    model.set_params(LinearModel__alpha=alpha)\n",
    "\n",
    "    # compute the cross validation error\n",
    "    cv_error = cross_val_score(model, X_train, Y_train, scoring = rmse_score, cv = 5).mean()\n",
    "    \n",
    "    cv_errors.append(cv_error)\n",
    "    \n",
    "best_alpha_lasso = alpha_arr[np.argmin(cv_errors)]\n",
    "\n",
    "plt.plot(alpha_arr, cv_errors)\n",
    "plt.title('Cross validation error over alpha')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Cross validation error')\n",
    "plt.show()\n",
    "\n",
    "print(f\"The best alpha value is {best_alpha_lasso}\")\n",
    "print(f\"Cross validation error for the best alpha value is {cv_errors[np.argmin(cv_errors)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q3\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Question 4\n",
    "In the previous questions, we have used cross validation to select the best $\\alpha$'s for ridge regression and lasso regression. Based on this, select the best model with the smallest cross validation error out of all the models and hyperparameters we have considered (either lasso regression with its best $\\alpha$ or ridge regression with its best $\\alpha$). Train this model using the **entire training set** and evaluate the performance of the model on the test set.\n",
    "\n",
    "**Note: If you are using Lasso for your final model, put `max_iter = 1000000` as an argument to `Lasso()`. The reason behind this is that for small regularization parameters, the Lasso Linear Regularization solver takes more iterations to converge.**\n",
    "\n",
    "<!--\n",
    "BEGIN QUESTION\n",
    "name: q4\n",
    "-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test RMSE: 4.994121608671193\n"
     ]
    }
   ],
   "source": [
    "final_model = Lasso(alpha = best_alpha_lasso, max_iter = 1000000)\n",
    "\n",
    "# Fit `final_model`\n",
    "final_model.fit(X_train_normalized, Y_train)\n",
    "\n",
    "# Error of prediction\n",
    "test_RMSE = rmse_score(final_model, X_test_normalized, Y_test)\n",
    "\n",
    "\n",
    "print('test RMSE:', test_RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "Running tests\n",
      "\n",
      "---------------------------------------------------------------------\n",
      "Test summary\n",
      "    Passed: 1\n",
      "    Failed: 0\n",
      "[ooooooooook] 100.0% passed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ok.grade(\"q4\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "Congratulations! You are finished with this assignment. Please don't forget to submit by 11:59pm PST on Monday, 04/13!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Submit\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output.\n",
    "**Please save before submitting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to submit.\n",
    "ok.submit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
